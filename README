- Cloud Run is a fully-managed compute platform that is suitable for deploying containerized applications
- Cloud Run allows user to write their script based on the user's favorite programming language then push it and package it as a container with Cloud Build. Compare with Cloud Function which only supports one request at a time, Cloud Run is able to be configured to support multiple concurrent requests on a single container instance which allows to save time and save cost.


##################### Virtual Env
conda create -yn tcc-env 'python=3.7'
conda activate tcc-env 
pip install -r requirements.txt
pip install jupyter


jupyter notebook

##################### Google Cloud
# Create the cluster - to run in Cloud Shell
gcloud config set project tcc-lucas-pierre
gcloud compute zones list | grep us-east1
gcloud config set compute/region us-east1

gcloud container clusters create-auto tcc-cluster \
    --project=tcc-lucas-pierre \
    --region=us-east1

# https://cloud.google.com/sdk/gcloud/reference/container/clusters/create-auto#--network

# to delete it:
gcloud container clusters delete --region=us-east1 tcc-cluster


gcloud container clusters get-credentials tcc-cluster --region us-east1

# Deploy an application to the cluster

## Create the Deployment
kubectl create deployment hello-server \
    --image=us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0

## Expose the Deployment
kubectl expose deployment hello-server --type LoadBalancer --port 80 --target-port 8080


# Deploy the container in Kubernetes cluster
docker build -t gcr.io/tcc-lucas-pierre/yolo:v2 .
docker run -it -p 80:80 gcr.io/tcc-lucas-pierre/yolo:v2

## Third command is pushing our container to a registry which we prefer not Docker Hub, Google Container Registry (gcr.io)
docker push gcr.io/tcc-lucas-pierre/yolo:v2

## If you are not logged in before third command, open terminal, type
gcloud init
gcloud services enable containerregistry.googleapis.com
gcloud auth login
gcloud auth configure-docker


If you wanna see the all running nodes you can type:
kubectl get nodes -A

kubectl apply -f yolo/build.yaml
kubectl get deployments
kubectl expose deployment hello-server --type LoadBalancer --port 80 --target-port 8080


##################### Prepare data folders to test client
Download these files and put them in a folder located at the path DATASET_PATH
.
└── fashion-dataset
    ├── images
    │   ├── 10000.jpg
    │   ├── 10001.jpg
    │   └── 10003.jpg
    ├── images.csv
    ├── styles
    │   ├── 10000.json
    │   ├── 10001.json
    │   └── 10003.json
    └── styles.csv


##################### Makefile deploy locally
make build

# built these images:
tcc-mba-ml-in-prod_client             latest    31e9c16c1253   2 hours ago    1.33GB
tcc-mba-ml-in-prod_api                latest    9a0fb3655b0a   2 hours ago    196MB
tcc-mba-ml-in-prod_inference_worker   latest    c05ea157c0c7   2 hours ago    6.89GB
tcc-mba-ml-in-prod_imagery_worker     latest    5844237a57ef   3 hours ago    853MB
tcc-mba-ml-in-prod_database           latest    09be6b798c6e   3 hours ago    315MB

make up

# ran these containers:
a1cd3095e929   tcc-mba-ml-in-prod_client             "tini -g -- start-no…"   18 seconds ago   Up 16 seconds (healthy)   0.0.0.0:8888->8888/tcp                                                                                       fashion-client
fb9345a032b9   tcc-mba-ml-in-prod_api                "/bin/sh -c 'uvicorn…"   19 seconds ago   Up 17 seconds             5672/tcp, 0.0.0.0:5000->5000/tcp, 6379/tcp                                                                   fashion-api
16c101cf0498   tcc-mba-ml-in-prod_inference_worker   "celery worker -A wo…"   20 seconds ago   Up 18 seconds             5672/tcp, 6379/tcp                                                                                           celery-inference
5b808dcf16f0   tcc-mba-ml-in-prod_imagery_worker     "celery worker -A wo…"   21 seconds ago   Up 19 seconds             5672/tcp, 6379/tcp                                                                                           celery-imagery
25f0fab01333   rabbitmq:3.8.18-management-alpine     "docker-entrypoint.s…"   22 seconds ago   Up 20 seconds             4369/tcp, 5671/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:8080->15672/tcp   celery-broker
d48437322926   tcc-mba-ml-in-prod_database           "docker-entrypoint.s…"   22 seconds ago   Up 20 seconds             5432/tcp                                                                                                     tcc-mba-ml-in-prod_database_1
60008d37e0ce   redis:6.2.4                           "docker-entrypoint.s…"   22 seconds ago   Up 20 seconds             0.0.0.0:6379->6379/tcp                                                                                       celery-backend
a3cbf1dcdd7e   localstack/localstack:0.12.13         "docker-entrypoint.sh"   22 seconds ago   Up 20 seconds             4566/tcp, 4571/tcp, 5678/tcp, 8080/tcp                                                                       tcc-mba-ml-in-prod_localstack_1
# these image names are because of the subfolder name tcc-mba-ml-in-prod and because of the name in docker-compose file (client, etc)
# but then we renamed them with image keyword in compose yaml file


# push to Google Container Registry
gcloud services enable containerregistry.googleapis.com
gcloud auth login

# (--pull so we always attempt to pull a newer version of the image)
docker-compose build --pull
docker-compose push

# convert docker-compose.yaml to files kubectl to be executed by kubectl
kompose convert
kompose convert --volumes hostPath
# this will generate these files:
WARN Volume mount on the host "/Users/pierre.krzisch/dev/perso/mba_ml_in_prod/tcc/tcc-mba-ml-in-prod/src/client/notebooks" isn't supported - ignoring path on the host 
INFO Network network is detected at Source, shall be converted to equivalent NetworkPolicy at Destination 
WARN Service "database" won't be created because 'ports' is not specified 
WARN Volume mount on the host "/database" isn't supported - ignoring path on the host 
INFO Network network is detected at Source, shall be converted to equivalent NetworkPolicy at Destination 
WARN Service "imagery_worker" won't be created because 'ports' is not specified 
WARN Volume mount on the host "/images" isn't supported - ignoring path on the host 
INFO Network network is detected at Source, shall be converted to equivalent NetworkPolicy at Destination 
WARN Service "inference_worker" won't be created because 'ports' is not specified 
WARN Volume mount on the host "/images" isn't supported - ignoring path on the host 
INFO Network network is detected at Source, shall be converted to equivalent NetworkPolicy at Destination 
WARN Service "localstack" won't be created because 'ports' is not specified 
WARN Volume mount on the host "/images" isn't supported - ignoring path on the host 
WARN Volume mount on the host "/Users/pierre.krzisch/dev/perso/mba_ml_in_prod/tcc/tcc-mba-ml-in-prod/src/localstack" isn't supported - ignoring path on the host 


# to execute in Google Cloud editor:
kubectl apply -f api-deployment.yaml,api-service.yaml,backend-deployment.yaml,backend-service.yaml,broker-deployment.yaml,broker-service.yaml,client-claim0-persistentvolumeclaim.yaml,client-deployment.yaml,client-service.yaml,database-claim0-persistentvolumeclaim.yaml,database-deployment.yaml,imagery-worker-claim0-persistentvolumeclaim.yaml,imagery-worker-deployment.yaml,inference-worker-claim0-persistentvolumeclaim.yaml,inference-worker-deployment.yaml,localstack-claim0-persistentvolumeclaim.yaml,localstack-claim1-persistentvolumeclaim.yaml,localstack-deployment.yaml,network-networkpolicy.yaml,variables-env-configmap.yaml

# Output:
deployment.apps/api created
service/api created

deployment.apps/backend created
service/backend created

deployment.apps/broker created
service/broker created
persistentvolumeclaim/client-claim0 created

deployment.apps/client created
service/client created
persistentvolumeclaim/database-claim0 created

deployment.apps/database created
persistentvolumeclaim/imagery-worker-claim0 created

deployment.apps/imagery-worker created
persistentvolumeclaim/inference-worker-claim0 created

deployment.apps/inference-worker created
persistentvolumeclaim/localstack-claim0 created
persistentvolumeclaim/localstack-claim1 created

deployment.apps/localstack created
networkpolicy.networking.k8s.io/network created
configmap/variables-env created

# check created objects
kubectl get nodes -A
kubectl get deployments
kubectl get services
kubectl get pod -n default
kubectl get networkpolicy

debug pod in CrashLoopBackOff status
kubectl describe pod -n default database-546f6fbf66-9h99c

# how to deal with insufficient resource problem:
# https://containersolutions.github.io/runbooks/posts/kubernetes/0-nodes-available-insufficient/

kubectl describe nodes
# check nodes resources usage
kubectl top nodes

kubectl describe node gk3-tcc-cluster-nap-x19d6wp9-c937c85b-llwp | less

kubectl get pods
kubectl logs client-567dcf5cff-grxxf


kubectl delete -f tcc-mba-ml-in-prod/client-deployment.yaml
kubectl apply -f tcc-mba-ml-in-prod/client-deployment.yaml

kubectl delete -f tcc-mba-ml-in-prod/network-networkpolicy.yaml
kubectl apply -f tcc-mba-ml-in-prod/network-networkpolicy.yaml

kubectl delete -f tcc-mba-ml-in-prod/localstack-deployment.yaml
kubectl apply -f tcc-mba-ml-in-prod/localstack-deployment.yaml

TODO:
solve problem
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "database" to address: Name or service not known
in imagery-worker-deployment.yaml


kubectl describe pod -n default database-6cd55c4bb6-5pbnd


##################### Step 1 - Deploy API locally
cd ~/dev/perso/mba_ml_in_prod/tcc/tcc-mba-ml-in-prod/src/api
docker build -t gcr.io/tcc-lucas-pierre/api .
docker run -it -p 5000:5000 gcr.io/tcc-lucas-pierre/api

# Third command is pushing our container to a registry which we prefer not Docker Hub, Google Container Registry (gcr.io)
docker push gcr.io/tcc-lucas-pierre/tcc-api

##################### Step 2 - Deploy database locally
cd ~/dev/perso/mba_ml_in_prod/tcc/tcc-mba-ml-in-prod/src/database
docker build -t gcr.io/tcc-lucas-pierre/database .
docker run -it -p 5000:5000 gcr.io/tcc-lucas-pierre/database

# Third command is pushing our container to a registry which we prefer not Docker Hub, Google Container Registry (gcr.io)
docker push gcr.io/tcc-lucas-pierre/database


# How to add data in PVC when pode related to it doesn't exist yet?


# Use the COPY FROM STDIN command to import data from a text or CSV file into a PostgreSQL-dialect database
cat mydata.csv | psql -h localhost \
  -c "COPY mytable FROM STDIN WITH (FORMAT csv, ESCAPE '~', HEADER TRUE);"


# TODO:
Except for Postrgesql, copy all files, declared via DATASET_PATH in docker compose, to mounted PVC with this command line:
# copy data.csv to the pod that uses the persistent volume:
kubectl cp tmp/database/data.csv default/database-6cd55c4bb6-qdxpz:/tmp/database/data.csv


# TODO: solve Problem with imagery-worker pod that cannot find database - when create_engine(DATABASE_URL)
    raise exception
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
    connection = pool._invoke_creator(self)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 508, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)




##################### Some code to interact with

# Localstack
s3 = s3fs.S3FileSystem(client_kwargs={'endpoint_url': f'http://{os.getenv("S3_HOST")}:4566'})

list(s3.walk("fashion-datasets"))
[('fashion-datasets', ['dataset-v1'], []),
 ('fashion-datasets/dataset-v1', [], ['10000.jpg', '10001.jpg', '10003.jpg'])]



##################### FLOW
flow:
- request for a specific cloth 
-- "{API_ENDPOINT}/filter"
--- src/api/main.py
@app.post("/filter", status_code=201)
task = tasks.send_task(name="filter",...
# this creates a task using Celery
--- src/workers/imagery/worker.py
@imagery.task(bind=True, name="filter")
result = psql.filter_products(query=query)...
upload(result=result, s3_obj=s3_obj, s3_target=s3_target)...
# this retrieves required data from PostrgeSQL and then
# this uploads the metadata + the processed data to a new s3 folder named after task's id

-- '{API_ENDPOINT}/task/{task_id}'
--- src/api/main.py
@app.get("/task/{task_id}", status_code=200)
async wait for the result of the task previously sent to Celery


##################### QUESTIONS
# QUESTION
why do we do this?
awslocal s3 sync /tmp/localstack/dataset s3://fashion-datasets/dataset-v1
